import asyncio
import aiohttp
import aiofiles
import os
import re

API_URL = "https://api.pokemontcg.io/v2/cards"
OUTPUT_DIR = "card_images"
PAGE_SIZE = 250
MAX_RETRIES = 5
PAGE_DELAY = 1
MAX_CONCURRENT_DOWNLOADS = 10

SEM = asyncio.Semaphore(MAX_CONCURRENT_DOWNLOADS)

def sanitize(text):
    return re.sub(r'[<>:"/\\|?*]', '', text)

async def fetch_page(session, page):
    params = {
        "page": page,
        "pageSize": PAGE_SIZE
    }

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            async with session.get(API_URL, params=params, timeout=30) as resp:
                resp.raise_for_status()
                return await resp.json()
        except Exception:
            if attempt == MAX_RETRIES:
                raise
            await asyncio.sleep(2)

async def download_image(session, url, path):
    async with SEM:
        async with session.get(url, timeout=30) as resp:
            resp.raise_for_status()
            data = await resp.read()
        async with aiofiles.open(path, "wb") as f:
            await f.write(data)

async def scrape_all_cards():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    timeout = aiohttp.ClientTimeout(total=60)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        page = 1
        total_downloaded = 0

        while True:
            data = await fetch_page(session, page)
            cards = data.get("data", [])

            if not cards:
                break

            tasks = []

            for card in cards:
                try:
                    name = sanitize(card["name"])
                    number = sanitize(card["number"])
                    set_name = sanitize(card["set"]["name"])
                    image_url = card["images"]["small"]

                    set_dir = os.path.join(OUTPUT_DIR, set_name)
                    os.makedirs(set_dir, exist_ok=True)

                    filename = f"{name}_{number}.png"
                    filepath = os.path.join(set_dir, filename)

                    if os.path.exists(filepath):
                        continue

                    tasks.append(download_image(session, image_url, filepath))
                    total_downloaded += 1
                except KeyError:
                    continue

            if tasks:
                await asyncio.gather(*tasks)

            page += 1
            await asyncio.sleep(PAGE_DELAY)

if __name__ == "__main__":
    asyncio.run(scrape_all_cards())
